# evaluation.py

import json
import argparse # 1. å¯¼å…¥ argparse æ¨¡å—ï¼Œç”¨äºå¤„ç†å‘½ä»¤è¡Œå‚æ•°
from sentence_transformers.cross_encoder import CrossEncoder
from sentence_transformers.cross_encoder.evaluation import CrossEncoderRerankingEvaluator

def load_samples(dataset_path: str) -> list:
    """
    ä» .jsonl æ–‡ä»¶åŠ è½½æ•°æ®é›†ã€‚

    Args:
        dataset_path (str): æ•°æ®é›†æ–‡ä»¶çš„è·¯å¾„ã€‚

    Returns:
        list: åŒ…å«æ ·æœ¬çš„åˆ—è¡¨ã€‚
    """
    samples = []
    print(f"æ­£åœ¨ä» {dataset_path} åŠ è½½æ•°æ®é›†...")
    with open(dataset_path, 'r', encoding='utf-8') as f:
        for line in f:
            samples.append(json.loads(line))
    print(f"åŠ è½½å®Œæˆï¼å…± {len(samples)} æ¡æ ·æœ¬ã€‚")
    return samples

def evaluate_model(model_path: str, samples: list) -> dict:
    """
    åŠ è½½æ¨¡å‹å¹¶æ‰§è¡Œè¯„ä¼°ã€‚

    Args:
        model_path (str): CrossEncoder æ¨¡å‹çš„è·¯å¾„ã€‚
        samples (list): ç”¨äºè¯„ä¼°çš„æ ·æœ¬åˆ—è¡¨ã€‚

    Returns:
        dict: åŒ…å«è¯„ä¼°ç»“æœï¼ˆMAP, MRR, NDCGï¼‰çš„å­—å…¸ã€‚
    """
    print(f"\n--- æ­£åœ¨åŠ è½½å¹¶è¯„ä¼°æ¨¡å‹: {model_path} ---")
    model = CrossEncoder(model_path)
    
    evaluator = CrossEncoderRerankingEvaluator(
        samples,
        at_k=10,
        name='test-evaluation',
        show_progress_bar=True
    )
    
    return evaluator(model)

def print_results(model_name: str, results: dict):
    """
    æ ¼å¼åŒ–æ‰“å°å•ä¸ªæ¨¡å‹çš„è¯„ä¼°ç»“æœã€‚
    """
    print(f"\nã€{model_name}ã€‘æ¨¡å‹æ€§èƒ½:")
    for key, value in results.items():
        metric_name = key.split('_')[-1] # ä» 'test-evaluation_map' ä¸­æå– 'map'
        print(f"  - {metric_name.upper()}: {value:.6f}")

def compare_and_print_changes(results_before: dict, results_after: dict):
    """
    å¯¹æ¯”ä¸¤ä¸ªæ¨¡å‹çš„ç»“æœå¹¶æ‰“å°æ€§èƒ½å˜åŒ–ã€‚
    """
    print("\n" + "="*50)
    print("ğŸš€ æ€§èƒ½å˜åŒ–åˆ†æ (è’¸é¦å vs. è’¸é¦å‰)")
    print("="*50)

    for key in results_before:
        metric_name = key.split('_')[-1]
        score_before = results_before[key]
        score_after = results_after[key]
        
        absolute_change = score_after - score_before
        relative_change = (absolute_change / score_before) * 100 if score_before != 0 else float('inf')
        
        change_sign = "â†‘" if absolute_change >= 0 else "â†“"
        
        print(f"æŒ‡æ ‡ [{metric_name.upper()}]:")
        print(f"  - ç»å¯¹æå‡: {absolute_change:+.6f}")
        print(f"  - ç›¸å¯¹æå‡: {relative_change:+.2f}% {change_sign}")

# 2. ä½¿ç”¨ if __name__ == "__main__": ç»“æ„ï¼Œè¿™æ˜¯ Python è„šæœ¬çš„æ ‡å‡†å…¥å£ç‚¹
if __name__ == "__main__":
    # 3. å®šä¹‰å‘½ä»¤è¡Œå‚æ•°è§£æå™¨
    parser = argparse.ArgumentParser(description="è¯„ä¼°å’Œå¯¹æ¯”æ¨¡å‹è’¸é¦å‰åçš„ Reranker æ€§èƒ½ã€‚")
    parser.add_argument(
      '--model_before_path', 
      type=str, 
      default='/root/autodl-tmp/bge-reranker-v2-m3', 
      required=False, 
      help='è’¸é¦å‰ï¼ˆåŸå§‹ï¼‰æ¨¡å‹çš„è·¯å¾„ã€‚'
    )
    parser.add_argument(
      '--model_after_path', 
      type=str, 
      default='output/checkpoint-1217', 
      required=False,
      help='è’¸é¦åï¼ˆå¾®è°ƒï¼‰æ¨¡å‹çš„è·¯å¾„ã€‚'
    )
    parser.add_argument(
      '--dataset_path', 
      type=str, 
      default='test.jsonl', 
      required=False, 
      help='test.jsonl æ•°æ®é›†æ–‡ä»¶çš„è·¯å¾„ã€‚'
    )
    args = parser.parse_args()

    # ä»æ–‡ä»¶ä¸­åŠ è½½æ•°æ®
    samples = load_samples(args.dataset_path)

    # è¯„ä¼°è’¸é¦å‰çš„æ¨¡å‹
    results_before = evaluate_model(args.model_before_path, samples)
    
    # è¯„ä¼°è’¸é¦åçš„æ¨¡å‹
    results_after = evaluate_model(args.model_after_path, samples)

    # æ‰“å°ç‹¬ç«‹çš„ç»“æœ
    print("\n\n" + "="*50)
    print("âœ… æœ€ç»ˆè¯„ä¼°ç»“æœæ±‡æ€»")
    print("="*50)
    print_results("è’¸é¦å‰", results_before)
    print_results("è’¸é¦å", results_after)
    
    # å¯¹æ¯”å¹¶æ‰“å°æ€§èƒ½å˜åŒ–
    compare_and_print_changes(results_before, results_after)

    print("\nè¯„ä¼°å®Œæˆï¼âœ¨")