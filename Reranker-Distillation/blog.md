# å°†é‡æ’åºå¤§æ¨¡å‹Qwen3-Reranker-8Bçš„çŸ¥è¯†è’¸é¦åˆ°å°æ¨¡å‹BGE-reranker-m3-v2ä¸Š
BGE-reranker-m3-v2æ˜¯ä¸€ä¸ªéå¸¸å¥½ç”¨çš„é‡æ’åºæ¨¡å‹ï¼Œåœ¨RAGï¼ˆæ£€ç´¢å¢å¼ºç”Ÿæˆï¼‰ä¸­ç”¨äºè¿›ä¸€æ­¥ä¼˜åŒ–æ£€ç´¢å‡ºçš„æ–‡æ¡£ã€‚ä½†æ˜¯æœ‰ä¸€ä¸ªç—›ç‚¹æ˜¯ç”¨å¤§æ¨¡å‹åˆæˆã€ç”šè‡³äººå·¥æ ‡æ³¨queryã€positiveã€negativeä¸‰å…ƒç»„æ•°æ®ç”¨äºè®­ç»ƒå¾®è°ƒä¼šæ¯”è¾ƒéº»çƒ¦ä¸”æˆæœ¬è¾ƒé«˜ã€‚æœ€è¿‘é˜¿é‡Œäº‘å‘å¸ƒäº†Qwen3-rerankerç³»åˆ—SOTAé‡æ’åºæ¨¡å‹ï¼Œæœ¬æ–‡å°è¯•ä¸€ä¸‹ç”¨æœ€å¼ºçš„8Bå¤§æ¨¡å‹Qwen3-reranker-8Bæ¥çŸ¥è¯†è’¸é¦0.6Bçš„BGE-reranker-m3-v2å°æ¨¡å‹ï¼Œå–å¾—äº†XX%çš„æå‡ã€‚

## å·¥å…·ï¼š
### å­¦ç”Ÿæ¨¡å‹ï¼šBGE-reranker-m3-v2
æ•™å¸ˆæ¨¡å‹: Qwen3-reranker-8B  
è®­ç»ƒ/è¯„æµ‹æ¡†æ¶ï¼šsentence-transformer  
æ•°æ®é›†ï¼šMTEB/stackoverflowdupquestions-reranking  
æ¨ç†æ¡†æ¶ï¼švLLM

## è®­ç»ƒæ–¹æ³•ï¼š
ç›®æ ‡æ˜¯è®©å­¦ç”Ÿæ¨¡å‹è®¡ç®—ï¼ˆæŸ¥è¯¢ï¼Œæ›´ç›¸å…³æ–‡æ¡£ï¼‰çš„ç›¸å…³æ€§åˆ†æ•°å’Œï¼ˆæŸ¥è¯¢ï¼Œæ›´ä¸ç›¸å…³æ–‡æ¡£ï¼‰çš„ç›¸å…³æ€§åˆ†æ•°çš„å·®å€¼ä¸æ•™å¸ˆæ¨¡å‹ç»™è¿™ä¸¤ä¸ªpairè®¡ç®—çš„åˆ†æ•°çš„å·®å€¼æ¥è¿‘ã€‚
loss å‡½æ•°æ˜¯Margin-MSEã€‚å…¬å¼å¦‚ä¸‹ï¼š
```
L (ğ‘„, ğ‘ƒ+, ğ‘ƒâˆ’) = MSE(ğ‘€ğ‘  (ğ‘„, ğ‘ƒ+) âˆ’ ğ‘€ğ‘  (ğ‘„, ğ‘ƒâˆ’), ğ‘€ğ‘¡ (ğ‘„, ğ‘ƒ+) âˆ’ ğ‘€ğ‘¡ (ğ‘„, ğ‘ƒâˆ’))
```
ğ‘„æ˜¯æŸ¥è¯¢ï¼Œğ‘ƒ+æ˜¯ç›¸å…³æ€§æ›´é«˜çš„æ–‡æ¡£ï¼Œğ‘ƒâˆ’æ˜¯ç›¸å…³æ€§æ›´ä½çš„æ–‡æ¡£ã€‚ğ‘€ğ‘¡()ã€ ğ‘€ğ‘ ()åˆ†åˆ«æ˜¯æ•™å¸ˆæ¨¡å‹è®¡ç®—çš„ç›¸å…³æ€§åˆ†æ•°ã€‚MSE()æ˜¯å‡æ–¹è¯¯å·®ã€‚

## ç¯å¢ƒï¼š
ä»¥ä¸‹æ˜¯æˆ‘çš„ç¯å¢ƒä¸­ç›¸å…³çš„å‡ ä¸ªåº“çš„ç‰ˆæœ¬ï¼Œå¦‚æœå…¶ä¸­ä½ æ²¡æœ‰å®‰è£…ï¼Œå¯ä»¥ç›´æ¥ä½¿ç”¨pip install XXXå®‰è£…ã€‚
```
Package                                  Version                  
---------------------------------------- ------------------------ 
torch                                    2.6.0
sentence-transformers                    5.0.0
transformers                             4.53.1
vllm                                     0.8.4
```

## å®ç°ï¼š
### ç¬¬ä¸€æ­¥ï¼šä¸‹è½½æ¨¡å‹å’Œæ•°æ®é›†
#### 1.ä¸‹è½½bge-reranker-v2-m3æ¨¡å‹
```
#æ¨¡å‹ä¸‹è½½
from modelscope import snapshot_download
model_dir = snapshot_download('BAAI/bge-reranker-v2-m3')
```
#### 2.ä¸‹è½½Qwen3-8B-rerankeræ¨¡å‹
```
#æ¨¡å‹ä¸‹è½½
from modelscope import snapshot_download
model_dir = snapshot_download('Qwen/Qwen3-Reranker-8B')
```
#### 3.ä¸‹è½½stackoverflowdupquestions-rerankingæ•°æ®é›†
è¿›å…¥https://www.modelscope.cn/datasets/MTEB/stackoverflowdupquestions-reranking/files  
æ‰‹åŠ¨ç‚¹å‡»ä¸‹è½½train.jsonl.gzã€test.jsonl.gzæ•°æ®åˆ°ä½ çš„å®éªŒç¯å¢ƒï¼ˆç›´æ¥æ‹‰å–æ•°æ®é›†å¯èƒ½ä¼šæœ‰BUGï¼‰  

æ‰§è¡Œä»¥ä¸‹å‘½ä»¤è§£å‹ï¼š
```
gunzip train.jsonl.gz
gunzip test.jsonl.gz
```

### ç¬¬äºŒæ­¥ï¼šå¤§æ¨¡å‹è®¡ç®—Logits

å®Œæ•´ä»£ç å¦‚ä¸‹ï¼š
```
import os
import json
import torch
from transformers import AutoTokenizer
from vllm import LLM, SamplingParams
from vllm.distributed.parallel_state import destroy_model_parallel
from tqdm import tqdm
import math
from vllm.inputs.data import TokensPrompt

# --- 1. è·¯å¾„å’Œå‚æ•°é…ç½® ---
os.environ['PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION'] = 'python'

# è¯·å°†æ¨¡å‹è·¯å¾„æ›´æ”¹ä¸ºæ‚¨å®é™…ä½¿ç”¨çš„ 8B æ¨¡å‹è·¯å¾„
model_path = '/root/autodl-tmp/Qwen3-Reranker-8B'

# å¯ä»¥åœ¨è¿™é‡Œæ·»åŠ ä»»æ„æ•°é‡çš„æ–‡ä»¶è·¯å¾„
input_dataset_paths = [
    'train.jsonl',
    'test.jsonl'
]

# ã€è‡ªåŠ¨ç”Ÿæˆã€‘è¾“å‡ºæ–‡ä»¶è·¯å¾„å°†æ ¹æ®è¾“å…¥æ–‡ä»¶è‡ªåŠ¨ç”Ÿæˆï¼Œæ— éœ€æ‰‹åŠ¨é…ç½®
# ä¾‹å¦‚ 'train.jsonl' -> 'train_distill_qwen3_8b_vLLMlogit.jsonl'
output_suffix = '_distill_qwen3_8b_vLLMlogit'

batch_size = 8  # ä½¿ç”¨ vllm å¯ä»¥æ˜¾è‘—å¢å¤§ batch_sizeï¼Œè¯·æ ¹æ®æ˜¾å­˜è°ƒæ•´

# --- 2. VLLM æ¨¡å‹åŠ è½½å’Œç›¸å…³é…ç½® ---

print(f"Using vLLM for inference.")
number_of_gpu = torch.cuda.device_count()
# åŠ è½½ tokenizer
tokenizer = AutoTokenizer.from_pretrained(model_path)
# ä½¿ç”¨ vLLM åŠ è½½æ¨¡å‹
# enable_prefix_caching=True å¯¹ rerank è¿™ç§å›ºå®šæ¨¡æ¿çš„ä»»åŠ¡æœ‰å¾ˆå¥½çš„åŠ é€Ÿæ•ˆæœ
print("Loading model with vLLM...")
model = LLM(
    model=model_path,
    tensor_parallel_size=number_of_gpu,
    max_model_len=8192,
    enable_prefix_caching=True,
    gpu_memory_utilization=0.9 # æ ¹æ®æ‚¨çš„æ˜¾å­˜æƒ…å†µè°ƒæ•´
)
print("Model loaded successfully.")

tokenizer.padding_side = "left"
tokenizer.pad_token = tokenizer.eos_token
suffix = "<|im_end|>\n<|im_start|>assistant\n<think>\n\n</think>\n\n"
max_length=8192
suffix_tokens = tokenizer.encode(suffix, add_special_tokens=False)
true_token = tokenizer("yes", add_special_tokens=False).input_ids[0]
false_token = tokenizer("no", add_special_tokens=False).input_ids[0]
sampling_params = SamplingParams(temperature=0,
    max_tokens=1,
    logprobs=20,
    allowed_token_ids=[true_token, false_token],
)

# ä½¿ç”¨Qwen3-rerankerå®˜æ–¹é»˜è®¤çš„æŒ‡ä»¤ (Instruction)
task_instruction = 'Given a web search query, retrieve relevant passages that answer the query'


# ç”Ÿæˆå¤§æ¨¡å‹messages
def format_and_tokenize_inputs(queries, docs, instruction):
    """ä½¿ç”¨ apply_chat_template æ ¼å¼åŒ–å¹¶ tokenize è¾“å…¥"""
    messages = []
    for query, doc in zip(queries, docs):
        # æ„å»ºç¬¦åˆ Qwen3 èŠå¤©æ ¼å¼çš„è¾“å…¥
        message = [
            {"role": "system", "content": "Judge whether the Document meets the requirements based on the Query and the Instruct provided. Note that the answer can only be \"yes\" or \"no\"."},
            {"role": "user", "content": f"<Instruct>: {instruction}\n<Query>: {query}\n<Document>: {doc}"}
        ]
        messages.append(message)
    templated_messages = tokenizer.apply_chat_template(
        messages,
        tokenize=True,
        add_generation_prompt=False,
        enable_thinking=False
    )
    processed_messages = [ele[:max_length] + suffix_tokens for ele in templated_messages]
    final_messages = [TokensPrompt(prompt_token_ids=ele) for ele in processed_messages]
    return final_messages

# ã€é€»è¾‘ä¸å˜ã€‘å‡½æ•°å®šä¹‰å®Œå…¨ä¿ç•™
def compute_scores_vllm(batch_queries, batch_docs):
    """è®¡ç®—åˆ†æ•°çš„å‡½æ•°"""
    tokenized_batch = format_and_tokenize_inputs(batch_queries, batch_docs, task_instruction)
    outputs = model.generate(tokenized_batch, sampling_params=sampling_params, use_tqdm=False)

    scores = []
    for i in range(len(outputs)):
        final_logprobs = outputs[i].outputs[0].logprobs[-1]

        # å¤„ç† "yes" token
        if true_token not in final_logprobs:
            true_logprob = -10.0 # ä½¿ç”¨æµ®ç‚¹æ•°
        else:
            true_logprob = final_logprobs[true_token].logprob

        # å¤„ç† "no" token
        if false_token not in final_logprobs:
            false_logprob = -10.0 # ä½¿ç”¨æµ®ç‚¹æ•°
        else:
            false_logprob = final_logprobs[false_token].logprob

        # è®¡ç®—logitç©ºé—´çš„å·®å€¼ (æ›´ç¨³å®šçš„è®¡ç®—æ–¹å¼)
        # logit = log(prob_yes) - log(prob_no)
        # ç”±äºlogprobæ˜¯log_softmaxçš„ç»“æœï¼Œç›´æ¥ç›¸å‡å³å¯
        logit_diff = true_logprob - false_logprob
        scores.append(logit_diff)

    return scores


# --- 3. æ•°æ®é›†ç”Ÿæˆä¸»å¾ªç¯ ---
# éå†è¾“å…¥æ–‡ä»¶åˆ—è¡¨ï¼Œå¯¹æ¯ä¸ªæ–‡ä»¶è¿›è¡Œå¤„ç†
for input_dataset_path in input_dataset_paths:
    # è‡ªåŠ¨æ„å»ºè¾“å‡ºæ–‡ä»¶è·¯å¾„
    base_name, extension = os.path.splitext(input_dataset_path)
    output_distill_path = f"{base_name}{output_suffix}{extension}"

    print("\n" + "="*80)
    print(f"Start processing file: {input_dataset_path}")
    print(f"Output will be saved to: {output_distill_path}")
    print("="*80 + "\n")

    # åŠ è½½å¹¶é‡æ„æ•°æ®é›†
    print(f"Loading and reformatting data from {input_dataset_path}...")
    try:
        with open(input_dataset_path, 'r', encoding='utf-8') as f:
            original_data = [json.loads(line) for line in f]
    except FileNotFoundError:
        print(f"Error: Input file not found at {input_dataset_path}. Skipping this file.")
        continue # å¦‚æœæ–‡ä»¶ä¸å­˜åœ¨ï¼Œè·³è¿‡å½“å‰å¾ªç¯

    # å°†åŸå§‹æ•°æ®æ ¼å¼è½¬æ¢ä¸ºæ‰å¹³çš„ (query, passage) å¯¹åˆ—è¡¨
    reformatted_data = []
    for item in tqdm(original_data[:], desc="Reformatting data"):
        query = item['query']
        # æ·»åŠ  positive passages
        for passage in item.get('positive', []):
            reformatted_data.append({'query': query, 'passage': passage})
        # æ·»åŠ  negative passages
        for passage in item.get('negative', []):
            reformatted_data.append({'query': query, 'passage': passage})

    print(f"Original items: {len(original_data)}, Reformatted to {len(reformatted_data)} query-passage pairs.")
    print(f"Starting to generate distillation data for {len(reformatted_data)} examples using vLLM...")

    # æ‰“å¼€è¾“å‡ºæ–‡ä»¶å‡†å¤‡å†™å…¥
    with open(output_distill_path, 'w', encoding='utf-8') as out_f:
        # ä½¿ç”¨ tqdm åˆ›å»ºè¿›åº¦æ¡ï¼Œåˆ†æ‰¹å¤„ç†ã€é‡æ„åã€‘çš„æ•°æ®
        for i in tqdm(range(0, len(reformatted_data), batch_size), desc=f"Generating Distill Data for {os.path.basename(input_dataset_path)}"):
            batch_data = reformatted_data[i:i+batch_size]
            # ä»é‡æ„åçš„æ•°æ®ä¸­æå– query å’Œ passage
            queries = [sample['query'] for sample in batch_data]
            documents = [sample['passage'] for sample in batch_data]

            # è®¡ç®—æ•™å¸ˆæ¨¡å‹çš„ logits (åœ¨æ­¤å¤„æ˜¯ logprobs çš„å·®å€¼)ï¼Œæ­¤å‡½æ•°è°ƒç”¨é€»è¾‘ä¸å˜
            scores_batch = compute_scores_vllm(queries, documents)

            # å°†ç»“æœå†™å…¥æ–°çš„ JSONL æ–‡ä»¶
            for j in range(len(batch_data)):
                final_score = scores_batch[j]

                # è¾“å‡ºæ ¼å¼ä¸åŸä»£ç ä¸€è‡´
                distill_sample = {
                    "query": queries[j],
                    "passage": documents[j],
                    "score": final_score
                }
                out_f.write(json.dumps(distill_sample, ensure_ascii=False) + '\n')
    
    print(f"\nDistillation for {input_dataset_path} successfully generated and saved to: {output_distill_path}")


# --- 4. æ¸…ç†èµ„æº ---
# åœ¨æ‰€æœ‰æ–‡ä»¶å¤„ç†å®Œæˆåå†æ¸…ç†æ¨¡å‹èµ„æº
destroy_model_parallel()
print("\nAll files have been processed. Model resources destroyed.")
print("Script finished.")
```

æ­£å¸¸æƒ…å†µä¼šæ‰“å°å¦‚ä¸‹logï¼Œè¡¨æ˜å®Œæˆç”Ÿæˆlogit:
```
Model loaded successfully.

================================================================================
Start processing file: train.jsonl
Output will be saved to: train_distill_qwen3_8b_vLLMlogit.jsonl
================================================================================

Loading and reformatting data from train.jsonl...
Reformatting data: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 19847/19847 [00:00<00:00, 99606.16it/s]
Original items: 19847, Reformatted to 593522 query-passage pairs.
Starting to generate distillation data for 593522 examples using vLLM...
Generating Distill Data for train.jsonl: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 74191/74191 [1:38:53<00:00, 12.50it/s]

Distillation for train.jsonl successfully generated and saved to: train_distill_qwen3_8b_vLLMlogit.jsonl

================================================================================
Start processing file: test.jsonl
Output will be saved to: test_distill_qwen3_8b_vLLMlogit.jsonl
================================================================================

Loading and reformatting data from test.jsonl...
Reformatting data: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2992/2992 [00:00<00:00, 112759.63it/s]
Original items: 2992, Reformatted to 89470 query-passage pairs.
Starting to generate distillation data for 89470 examples using vLLM...
Generating Distill Data for test.jsonl: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11184/11184 [14:53<00:00, 12.51it/s]

Distillation for test.jsonl successfully generated and saved to: test_distill_qwen3_8b_vLLMlogit.jsonl

All files have been processed. Model resources destroyed.
Script finished.
```

è¿™ä¸€æ­¥ä¼šç”Ÿæˆä¸¤ä¸ªjsonlæ–‡ä»¶
```
train_distill_qwen3_8b_vLLMlogit.jsonl
test_distill_qwen3_8b_vLLMlogit.jsonl
```
å†…å®¹å¤§è‡´å¦‚ä¸‹ï¼š
```
{"query": "String isNullOrEmpty in Java?", "passage": "Java equivalent of c# String.IsNullOrEmpty() and String.IsNullOrWhiteSpace()", "score": 0.7499999403953552}
......
```
### ç¬¬ä¸‰æ­¥ï¼šç”Ÿæˆqueryã€positiveã€negativeä¸‰å…ƒç»„æ•°æ®é›†

å®Œæ•´ä»£ç å¦‚ä¸‹ï¼š
```
import json
import itertools
import os
from collections import defaultdict
from tqdm import tqdm
import random # å¼•å…¥ random åº“

def convert_to_margin_mse_format_sampled(input_file_path: str,
                                         output_file_path: str,
                                         top_k: int = 5,
                                         num_negatives: int = 5):
    """
    å°† (query, passage, score) æ ¼å¼çš„æ•°æ®é›†è½¬æ¢ä¸º (query, positive, negative, score_diff) æ ¼å¼ï¼Œ
    å¹¶ä½¿ç”¨æ™ºèƒ½é‡‡æ ·ç­–ç•¥æ§åˆ¶æ•°æ®é›†å¤§å°ã€‚

    Args:
        input_file_path (str): è¾“å…¥çš„ .jsonl æ–‡ä»¶è·¯å¾„ã€‚
        output_file_path (str): è¾“å‡ºè½¬æ¢åæ ¼å¼çš„ .jsonl æ–‡ä»¶è·¯å¾„ã€‚
        top_k (int): å¯¹äºæ¯ä¸ªqueryï¼Œé€‰æ‹©åˆ†æ•°æœ€é«˜çš„kä¸ªpassageä½œä¸ºå€™é€‰æ­£ä¾‹ã€‚
        num_negatives (int): ä¸ºæ¯ä¸ªæ­£ä¾‹é€‰æ‹©çš„è´Ÿä¾‹æ•°é‡ã€‚
    """
    if not os.path.exists(input_file_path):
        print(f"é”™è¯¯: è¾“å…¥æ–‡ä»¶æœªæ‰¾åˆ°: {input_file_path}")
        return

    print(f"â–¶ï¸ å¼€å§‹å¤„ç†æ–‡ä»¶: {input_file_path}")
    print(f"   - é‡‡æ ·ç­–ç•¥: top_k={top_k}, num_negatives={num_negatives}")

    # 1. æŒ‰ query åˆ†ç»„
    query_to_passages = defaultdict(list)
    with open(input_file_path, 'r', encoding='utf-8') as f_in:
        for line in tqdm(f_in, desc="  - æ­¥éª¤1/3: è¯»å–å¹¶åˆ†ç»„æ•°æ®"):
            try:
                data = json.loads(line)
                query_to_passages[data['query']].append({
                    "passage": data['passage'],
                    "score": data['score']
                })
            except json.JSONDecodeError:
                print(f"  - è­¦å‘Š: è·³è¿‡æ ¼å¼é”™è¯¯çš„è¡Œ: {line.strip()}")
                continue

    print(f"  - æ‰¾åˆ°äº† {len(query_to_passages)} ä¸ªç‹¬ç«‹çš„ queryã€‚")

    # 2. æ’åºã€é‡‡æ ·å¹¶ç”Ÿæˆä¸‰å…ƒç»„
    triplets_count = 0
    with open(output_file_path, 'w', encoding='utf-8') as f_out:
        for query, passages in tqdm(query_to_passages.items(), desc="  - æ­¥éª¤2/3: æ’åºå¹¶ç”Ÿæˆé‡‡æ ·ä¸‰å…ƒç»„"):
            if len(passages) < 2:
                continue

            # æŒ‰åˆ†æ•°ä»é«˜åˆ°ä½æ’åº
            passages.sort(key=lambda x: x['score'], reverse=True)

            # é€‰æ‹© top_k ä½œä¸ºæ­£ä¾‹
            positive_candidates = passages[:top_k]

            for i, p_pos in enumerate(positive_candidates):
                # è´Ÿä¾‹æ± æ˜¯å½“å‰æ­£ä¾‹ä¹‹åçš„æ‰€æœ‰ passage
                negative_pool = passages[i+1:]
                if not negative_pool:
                    continue

                # é€‰æ‹© "hard negatives" (åˆ†æ•°æœ€æ¥è¿‘çš„)
                # å¦‚æœè´Ÿä¾‹æ± å¤§å°è¶…è¿‡ num_negativesï¼Œå°±ä»ä¸­é€‰æ‹©æœ€éš¾çš„ num_negatives ä¸ª
                # ä¹Ÿå¯ä»¥éšæœºé€‰æ‹©: negative_samples = random.sample(negative_pool, min(len(negative_pool), num_negatives))
                negative_samples = negative_pool[:num_negatives]

                for p_neg in negative_samples:
                    # ç¡®ä¿ passage å†…å®¹ä¸åŒ
                    if p_pos['passage'] == p_neg['passage']:
                        continue

                    score_diff = p_pos['score'] - p_neg['score']
                    
                    # åªæœ‰å½“åˆ†æ•°å·®ä¸ºæ­£æ—¶æ‰æ˜¯æœ‰æ„ä¹‰çš„pairï¼ˆæ’åºä¿è¯äº†è¿™ä¸€ç‚¹ï¼Œä½†ä»¥é˜²ä¸‡ä¸€ï¼‰
                    if score_diff > 0:
                        new_record = {
                            "query": query,
                            "positive": p_pos['passage'],
                            "negative": p_neg['passage'],
                            "score": score_diff
                        }
                        f_out.write(json.dumps(new_record, ensure_ascii=False) + '\n')
                        triplets_count += 1
    
    print(f"  - æ­¥éª¤3/3: è½¬æ¢å®Œæˆï¼")
    print(f"âœ… (é‡‡æ ·å) æ€»å…±ç”Ÿæˆäº† {triplets_count} ä¸ªä¸‰å…ƒç»„, å·²ä¿å­˜åˆ°: {output_file_path}")


if __name__ == '__main__':
    files_to_process = [
        "train_distill_qwen3_8b_vLLMlogit.jsonl",
        "test_distill_qwen3_8b_vLLMlogit.jsonl"
    ]

    for input_file in files_to_process:
        base_name, extension = os.path.splitext(input_file)
        # æ›´æ–°è¾“å‡ºæ–‡ä»¶åä»¥åæ˜ é‡‡æ ·æ–¹æ³•
        output_file = f"{base_name}_margin_sampled{extension}"
        
        # è¿è¡Œå¸¦æœ‰é‡‡æ ·é€»è¾‘çš„è½¬æ¢å‡½æ•°
        # ä½ å¯ä»¥è°ƒæ•´ top_k å’Œ num_negatives æ¥æ§åˆ¶æ•°æ®é›†å¤§å°å’Œè´¨é‡
        # ä¾‹å¦‚ï¼Œk=5, neg=5 -> æ¯ä¸ªqueryæœ€å¤šç”Ÿæˆ 5*5=25 ä¸ªæ ·æœ¬
        # ä¾‹å¦‚ï¼Œk=10, neg=3 -> æ¯ä¸ªqueryæœ€å¤šç”Ÿæˆ 10*3=30 ä¸ªæ ·æœ¬
        convert_to_margin_mse_format_sampled(input_file, output_file, top_k=8, num_negatives=4)
        print("-" * 50)
```
å®Œæˆåä¼šè¾“å‡ºå¦‚ä¸‹log:
```
â–¶ï¸ å¼€å§‹å¤„ç†æ–‡ä»¶: train_distill_qwen3_8b_vLLMlogit.jsonl
   - é‡‡æ ·ç­–ç•¥: top_k=8, num_negatives=4
  - æ­¥éª¤1/3: è¯»å–å¹¶åˆ†ç»„æ•°æ®: 593522it [00:02, 259600.82it/s]
  - æ‰¾åˆ°äº† 19820 ä¸ªç‹¬ç«‹çš„ queryã€‚
  - æ­¥éª¤2/3: æ’åºå¹¶ç”Ÿæˆé‡‡æ ·ä¸‰å…ƒç»„: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 19820/19820 [00:04<00:00, 4160.78it/s]
  - æ­¥éª¤3/3: è½¬æ¢å®Œæˆï¼
âœ… (é‡‡æ ·å) æ€»å…±ç”Ÿæˆäº† 623271 ä¸ªä¸‰å…ƒç»„, å·²ä¿å­˜åˆ°: train_distill_qwen3_8b_vLLMlogit_margin_sampled.jsonl
--------------------------------------------------
â–¶ï¸ å¼€å§‹å¤„ç†æ–‡ä»¶: test_distill_qwen3_8b_vLLMlogit.jsonl
   - é‡‡æ ·ç­–ç•¥: top_k=8, num_negatives=4
  - æ­¥éª¤1/3: è¯»å–å¹¶åˆ†ç»„æ•°æ®: 89470it [00:00, 255129.85it/s]
  - æ‰¾åˆ°äº† 2992 ä¸ªç‹¬ç«‹çš„ queryã€‚
  - æ­¥éª¤2/3: æ’åºå¹¶ç”Ÿæˆé‡‡æ ·ä¸‰å…ƒç»„: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2992/2992 [00:00<00:00, 4176.65it/s]
  - æ­¥éª¤3/3: è½¬æ¢å®Œæˆï¼
âœ… (é‡‡æ ·å) æ€»å…±ç”Ÿæˆäº† 94044 ä¸ªä¸‰å…ƒç»„, å·²ä¿å­˜åˆ°: test_distill_qwen3_8b_vLLMlogit_margin_sampled.jsonl
--------------------------------------------------
```
å¹¶ç”Ÿæˆå¦‚ä¸‹ä¸¤ä¸ªæ–‡ä»¶ï¼š
```
train_distill_qwen3_8b_vLLMlogit_margin_sampled.jsonl
test_distill_qwen3_8b_vLLMlogit_margin_sampled.jsonl
```
æ•°æ®é›†å†…å®¹å¤§è‡´å¦‚ä¸‹ï¼š
```
{"query": "String isNullOrEmpty in Java?", "positive": "Java equivalent of c# String.IsNullOrEmpty() and String.IsNullOrWhiteSpace()", "negative": "isLocalHost(String hostNameOrIpAddress) in Java", "score": 6.0777692794799805}
......
```

### ç¬¬å››æ­¥ï¼šè®­ç»ƒ
è®­ç»ƒè„šæœ¬train_kd_margin.pyå®Œæ•´ä»£ç å¦‚ä¸‹
```
import logging
import os
from dataclasses import dataclass, field

from datasets import Value, load_dataset
from sentence_transformers.cross_encoder import CrossEncoder
from sentence_transformers.cross_encoder.losses import MarginMSELoss
from sentence_transformers.cross_encoder.trainer import CrossEncoderTrainer
from sentence_transformers.cross_encoder.training_args import CrossEncoderTrainingArguments
from transformers import HfArgumentParser

# è®¾ç½®æ—¥å¿—è®°å½•
logging.basicConfig(
    format="%(asctime)s - %(message)s",
    datefmt="%Y-%m-%d %H:%M:%S",
    level=logging.INFO,
)
logger = logging.getLogger(__name__)

# ç¦ç”¨ W&B æ—¥å¿—è®°å½•
os.environ["WANDB_DISABLED"] = "true"


@dataclass
class ModelArguments:
    """
    ä¸æ¨¡å‹å’Œæ•°æ®ç›¸å…³çš„å‚æ•°
    """

    model_name_or_path: str = field(metadata={"help": "é¢„è®­ç»ƒæ¨¡å‹çš„è·¯å¾„æˆ– Hugging Face Hub ä¸Šçš„åç§°"})
    train_data: str = field(metadata={"help": "è®­ç»ƒæ•°æ®æ–‡ä»¶è·¯å¾„ (.jsonl)"})
    eval_data: str = field(metadata={"help": "è¯„ä¼°æ•°æ®æ–‡ä»¶è·¯å¾„ (.jsonl)"})
    max_length: int = field(
        default=512, metadata={"help": "æ¨¡å‹å¤„ç†çš„æœ€å¤§åºåˆ—é•¿åº¦ (query + passage)"}
    )


def main():
    # 1. è§£æå‘½ä»¤è¡Œå‚æ•°
    parser = HfArgumentParser((ModelArguments, CrossEncoderTrainingArguments))
    model_args, training_args = parser.parse_args_into_dataclasses()
    logger.info(f"æ¨¡å‹å‚æ•°: {model_args}")
    logger.info(f"è®­ç»ƒå‚æ•°: {training_args}")
    logger.info(f"è®­ç»ƒè„šæœ¬å·²é€‚é… MarginMSELossï¼Œå°†ä½¿ç”¨ (query, positive, negative) ä¸‰å…ƒç»„è¿›è¡Œè®­ç»ƒã€‚")


    # 2. åˆå§‹åŒ– CrossEncoder æ¨¡å‹
    # å¯¹äºå›å½’/è’¸é¦ä»»åŠ¡ï¼Œnum_labels è®¾ç½®ä¸º 1
    model = CrossEncoder(
        model_args.model_name_or_path,
        num_labels=1,
        max_length=model_args.max_length,
    )

    # 3. åŠ è½½å’Œé¢„å¤„ç†æ•°æ®é›†
    logger.info("æ­£åœ¨åŠ è½½æ•°æ®é›†...")
    # æ•°æ®é›†åº”åŒ…å« 'query', 'positive', 'negative', 'score' åˆ—
    train_dataset = load_dataset(
        "json", data_files=model_args.train_data
    )["train"]
    eval_dataset = load_dataset(
        "json", data_files=model_args.eval_data
    )["train"]

    # ç¡®ä¿ 'score' åˆ—æ˜¯ float ç±»å‹
    train_dataset = train_dataset.cast_column("score", Value("float32"))
    eval_dataset = eval_dataset.cast_column("score", Value("float32"))

    logger.info(f"è®­ç»ƒé›†æ ·æœ¬æ•°: {len(train_dataset)}")
    logger.info(f"è¯„ä¼°é›†æ ·æœ¬æ•°: {len(eval_dataset)}")
    logger.info(f"è®­ç»ƒé›†çš„ä¸€ä¸ªæ ·æœ¬: {train_dataset[0]}")


    # 4. å®šä¹‰æŸå¤±å‡½æ•°
    # MarginMSELoss ç”¨äºçŸ¥è¯†è’¸é¦ï¼Œå®ƒå¤„ç† (query, positive, negative) ä¸‰å…ƒç»„
    loss = MarginMSELoss(model)

    # 5. åˆå§‹åŒ– Trainer
    # æ³¨æ„ï¼šæˆ‘ä»¬æ²¡æœ‰æä¾›è‡ªå®šä¹‰çš„ evaluatorï¼Œå› ä¸º CECorrelationEvaluator ä¸é€‚ç”¨ã€‚
    # Trainer å°†é»˜è®¤åœ¨è¯„ä¼°é›†ä¸Šè®¡ç®—æŸå¤±ï¼ˆlossï¼‰ä½œä¸ºè¯„ä¼°æŒ‡æ ‡ã€‚
    # å› æ­¤ï¼Œ'metric_for_best_model' åº”è®¾ç½®ä¸º 'eval_loss'ã€‚
    trainer = CrossEncoderTrainer(
        model=model,
        args=training_args,
        train_dataset=train_dataset,
        eval_dataset=eval_dataset,
        loss=loss,
    )

    # 6. å¼€å§‹è®­ç»ƒ
    logger.info("å¼€å§‹æ¨¡å‹è®­ç»ƒ...")
    trainer.train()

    # 7. ä¿å­˜æœ€ç»ˆæ¨¡å‹
    logger.info("è®­ç»ƒå®Œæˆï¼Œä¿å­˜æœ€ç»ˆæ¨¡å‹...")
    model.save_pretrained(training_args.output_dir)
    logger.info(f"æ¨¡å‹å·²ä¿å­˜è‡³: {training_args.output_dir}")


if __name__ == "__main__":
    main()
```

åœ¨ç»ˆç«¯æ‰§è¡Œä»¥ä¸‹å‘½ä»¤å¼€å§‹è®­ç»ƒï¼š
```
torchrun --nproc_per_node 1 train_kd_margin.py \
    --model_name_or_path your_path_to/bge-reranker-v2-m3 \
    --train_data your_path_to/train_distill_qwen3_8b_vLLMlogit_margin_sampled.jsonl \
    --eval_data your_path_to/test_distill_qwen3_8b_vLLMlogit_margin_sampled.jsonl \
    --output_dir output \
    --max_length 4096 \
    --num_train_epochs 1 \
    --per_device_train_batch_size 16 \
    --gradient_accumulation_steps 32 \
    --learning_rate 2e-5 \
    --warmup_ratio 0.05 \
    --logging_steps 1 \
    --save_strategy "steps" \
    --save_steps 10000 \
    --eval_strategy "steps" \
    --eval_steps 10000 \
    --save_total_limit 2 \
    --bf16 \
    --load_best_model_at_end \
    --save_only_mode true \
    --metric_for_best_model "eval_loss"
```
logå¦‚ä¸‹ï¼š
```
2025-07-11 13:29:41 - è®­ç»ƒè„šæœ¬å·²é€‚é… MarginMSELossï¼Œå°†ä½¿ç”¨ (query, positive, negative) ä¸‰å…ƒç»„è¿›è¡Œè®­ç»ƒã€‚
2025-07-11 13:29:41 - Use pytorch device: cuda:0
2025-07-11 13:29:42 - æ­£åœ¨åŠ è½½æ•°æ®é›†...
2025-07-11 13:29:43 - è®­ç»ƒé›†æ ·æœ¬æ•°: 623271
2025-07-11 13:29:43 - è¯„ä¼°é›†æ ·æœ¬æ•°: 94044
2025-07-11 13:29:43 - è®­ç»ƒé›†çš„ä¸€ä¸ªæ ·æœ¬: {'query': 'Java launch error selection does not contain a main type', 'positive': 'Eclipse: "selection does not contain a main type" error when main function exists', 'negative': 'Eclipse Java Launch Error: Selection does not contain a main type', 'score': 0.2500000596046448}
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
[2025-07-11 13:29:43,663] [INFO] [real_accelerator.py:219:get_accelerator] Setting ds_accelerator to cuda (auto detect)
2025-07-11 13:29:44 - å¼€å§‹æ¨¡å‹è®­ç»ƒ...
  0%|          | 1/1218 [00:03<1:16:36,  3.78s/it]
{'loss': 9.0142, 'grad_norm': 205.3222198486328, 'learning_rate': 0.0, 'epoch': 0.0}
......
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 1217/1218 [1:09:09<00:03,  3.41s/it]
{'loss': 0.733, 'grad_norm': 5.882925510406494, 'learning_rate': 3.4572169403630083e-08, 'epoch': 1.0}
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1218/1218 [1:09:10<00:00,  2.75s/it]2025-07-11 14:38:56 - Saving model checkpoint to output/checkpoint-1218
2025-07-11 14:38:56 - Save model to output/checkpoint-1218
{'loss': 0.217, 'grad_norm': 2.9422049522399902, 'learning_rate': 1.7286084701815042e-08, 'epoch': 1.0}
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1218/1218 [1:11:24<00:00,  3.52s/it]
2025-07-11 14:41:09 - è®­ç»ƒå®Œæˆï¼Œä¿å­˜æœ€ç»ˆæ¨¡å‹...
2025-07-11 14:41:09 - Save model to output
{'train_runtime': 4284.348, 'train_samples_per_second': 145.476, 'train_steps_per_second': 0.284, 'train_loss': 0.7906847017494524, 'epoch': 1.0}
2025-07-11 14:41:12 - æ¨¡å‹å·²ä¿å­˜è‡³: output
```

### ç¬¬äº”æ­¥ï¼šè¯„æµ‹
è¯„æµ‹ä»£ç å¦‚ä¸‹ï¼š
```
import json
from sentence_transformers.cross_encoder import CrossEncoder
from sentence_transformers.cross_encoder.evaluation import CrossEncoderRerankingEvaluator

# 1. æŒ‡å®šæ‚¨çš„æ¨¡å‹è·¯å¾„
model_path = 'your_path_to_checkpoint/checkpoint-XXXX'
model = CrossEncoder(model_path)

# 2. ä» test.jsonl åŠ è½½æ•°æ®é›†
samples = []
with open('your_path_to/test.jsonl', 'r') as f:
    for line in f:
        samples.append(json.loads(line))

# 3. åˆå§‹åŒ–è¯„ä¼°å™¨
# at_k=10 è¡¨ç¤ºè¯„ä¼°æŒ‡æ ‡ï¼ˆå¦‚ MRR å’Œ NDCGï¼‰å°†è®¡ç®—åˆ°å‰ 10 ä¸ªç»“æœ
evaluator = CrossEncoderRerankingEvaluator(
    samples, 
    at_k=10, 
    name='test-evaluation',
    show_progress_bar=True
)

# 4. è¿è¡Œè¯„ä¼°
# è¯„ä¼°å™¨ä¼šè®¡ç®—æ¨¡å‹å¯¹ "positive" å’Œ "negative" æ–‡æ¡£è¿›è¡Œé‡æ’åçš„æ€§èƒ½
# è¯„ä¼°æŒ‡æ ‡åŒ…æ‹¬ MAP, MRR@10, å’Œ NDCG@10
results = evaluator(model)
print("\n--- Returned Results Dictionary ---")
print(results)
```

åˆ†åˆ«å°†model_pathæ”¹ä¸ºåŸå§‹æ¨¡å‹å’Œè’¸é¦åçš„æ¨¡å‹æƒé‡çš„è·¯å¾„ï¼Œç»“æœå¦‚ä¸‹ï¼š  
è’¸é¦å‰ï¼š
```
--- Returned Results Dictionary ---
{'test-evaluation_map': 0.47206092285098944, 'test-evaluation_mrr@10': 0.4782342861386979, 'test-evaluation_ndcg@10': 0.5472842802832706}
```

è’¸é¦åï¼š
```
--- Returned Results Dictionary ---
{'test-evaluation_map': 0.5654945694133273, 'test-evaluation_mrr@10': 0.5738247761225702, 'test-evaluation_ndcg@10': 0.6386302876724329}
```

root@autodl-container-63984a89ed-fcc49336:~/autodl-tmp/finetune_bgereranker/stackover# python eval-github.py \
  --model_before_path /root/autodl-tmp/bge-reranker-v2-m3 \
  --model_after_path /root/autodl-tmp/finetune_bgereranker/stackover/output4/checkpoint-1218 \
  --dataset_path /root/autodl-tmp/dataset/stackoverflowdupquestions-reranking/test.jsonl

æ­£åœ¨ä» /root/autodl-tmp/dataset/stackoverflowdupquestions-reranking/test.jsonl åŠ è½½æ•°æ®é›†...
åŠ è½½å®Œæˆï¼å…± 2992 æ¡æ ·æœ¬ã€‚

--- æ­£åœ¨åŠ è½½å¹¶è¯„ä¼°æ¨¡å‹: /root/autodl-tmp/bge-reranker-v2-m3 ---
                                                                                                                                        
--- æ­£åœ¨åŠ è½½å¹¶è¯„ä¼°æ¨¡å‹: /root/autodl-tmp/finetune_bgereranker/stackover/output4/checkpoint-1218 ---
                                                                                                                                        

==================================================
âœ… æœ€ç»ˆè¯„ä¼°ç»“æœæ±‡æ€»
==================================================

ã€è’¸é¦å‰ã€‘æ¨¡å‹æ€§èƒ½:
  - MAP: 0.472061
  - MRR@10: 0.478234
  - NDCG@10: 0.547284

ã€è’¸é¦åã€‘æ¨¡å‹æ€§èƒ½:
  - MAP: 0.565495
  - MRR@10: 0.573825
  - NDCG@10: 0.638630

==================================================
ğŸš€ æ€§èƒ½å˜åŒ–åˆ†æ (è’¸é¦å vs. è’¸é¦å‰)
==================================================
æŒ‡æ ‡ [MAP]:
  - ç»å¯¹æå‡: +0.093434
  - ç›¸å¯¹æå‡: +19.79% â†‘
æŒ‡æ ‡ [MRR@10]:
  - ç»å¯¹æå‡: +0.095590
  - ç›¸å¯¹æå‡: +19.99% â†‘
æŒ‡æ ‡ [NDCG@10]:
  - ç»å¯¹æå‡: +0.091346
  - ç›¸å¯¹æå‡: +16.69% â†‘

è¯„ä¼°å®Œæˆï¼âœ¨